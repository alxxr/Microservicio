# ******************************  Metodo Funcional - Sin arquitectura hexagonal  ********************************

# from fastapi import FastAPI
# from transformers import AutoModelForSequenceClassification, AutoTokenizer
# from pydantic import BaseModel
# from typing import List, Dict, Any
# import torch


# # Start FastApi
# app = FastAPI()

# @app.get("/")
# async def root():
#     return {"message": "Welcome to my REST API"}


# # Receive the message with the correct model
# class Prospect(BaseModel):
#     prospect: str
#     messages: List[str]

# class Request(BaseModel):
#     prospects: List[Prospect]


# # Import the Hugging face library
# model_name = "cardiffnlp/twitter-roberta-base-sentiment-latest"
# tokenizer = AutoTokenizer.from_pretrained(model_name)
# model = AutoModelForSequenceClassification.from_pretrained(model_name)



# def preprocess_text(text: List[str]):
#     inputs = tokenizer(text, padding=True, truncation=True, return_tensors="pt")
#     input_ids = inputs["input_ids"]
#     attention_mask = inputs["attention_mask"]
#     return input_ids, attention_mask


# # Get the BFF request so we can parse it and return the name and score of each message with the correct model
# @app.post("/analyze_sentiment/")
# async def analyze_sentiment(request: Request):
#     texts = [message for prospect in request.prospects for message in prospect.messages]
#     input_ids, attention_mask = preprocess_text(texts)
#     with torch.no_grad():
#         outputs = model(input_ids, attention_mask=attention_mask)
#         probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)
    
#     # Lista para almacenar los resultados positivos y neutrales
#     positive_results = []
#     neutral_results = []
    
#     # Iterar sobre los prospectos y sus puntajes
#     for i, prospect in enumerate(request.prospects):
#         positive_score = probabilities[i, 2].item()  # Puntaje para sentimiento positivo
#         neutral_score = probabilities[i, 1].item()   # Puntaje para sentimiento neutral


#         positive_results.append({"prospect": prospect.prospect, "score": positive_score})
#         neutral_results.append({"prospect": prospect.prospect, "score": neutral_score})


#     # Formatear la salida seg√∫n el modelo requerido
#     output = {"positive": positive_results, "neutral": neutral_results}
#     return output

# if __name__ == "__main__":
#     import uvicorn
#     uvicorn.run(app, host="127.0.0.1", port=8000)



# ******************************  Metodo Funcional - Return sin formato  ********************************

# app = FastAPI()

# class Prospect(BaseModel):
#     prospect: str
#     messages: List[str]

# class Request(BaseModel):
#     prospects: List[Prospect]

# model_name = "cardiffnlp/twitter-roberta-base-sentiment-latest"
# tokenizer = AutoTokenizer.from_pretrained(model_name)
# model = AutoModelForSequenceClassification.from_pretrained(model_name)

# def preprocess_text(text: List[str]):
#     inputs = tokenizer(text, padding=True, truncation=True, return_tensors="pt")
#     input_ids = inputs["input_ids"]
#     attention_mask = inputs["attention_mask"]
#     return input_ids, attention_mask

# @app.post("/analyze_sentiment/")
# async def analyze_sentiment(request: Request):
#     texts = [message for prospect in request.prospects for message in prospect.messages]
#     input_ids, attention_mask = preprocess_text(texts)
#     with torch.no_grad():
#         outputs = model(input_ids, attention_mask=attention_mask)
#         probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)
#     sentiment_scores = {
#         "positive": probabilities[:, 2].tolist(),
#         "neutral": probabilities[:, 1].tolist(),
#         "negative": probabilities[:, 0].tolist(),
#     }
#     return sentiment_scores

# if __name__ == "__main__":
#     import uvicorn
#     uvicorn.run(app, host="127.0.0.1", port=8000)
